{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Music channels reporting\n",
    "------------------------\n",
    "\n",
    "Illustrated report on the contribution of YouTube music channels to my â€œmusical assetsâ€ over the last few years (from 2021 to today).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The aim of this notebook is quite simple: to summarize and illustrate the importance of music channels in their daily\n",
    " contribution to my personal listening. With a long-format listening process automated for almost 3 years at the time\n",
    "  of writing (January 7th, 2025), and a single-track listening process automated since January 1st, 2024\n",
    "  (accompanied\n",
    "  by metrics collection), it was time to create this new process, which this time will enable me to adjust daily listening sources.\n",
    "\n",
    "In other words, a new way of choosing which music channels to follow, or not. Channel selection will be based on the\n",
    "following playlists:\n",
    "\n",
    "* [ðŸ”‚ Re-listening](https://www.youtube.com/playlist?list=PLOMUdQFdS-XP8fi89uBQ5P01DN_9tGJHu) (Private): A playlist of all the re-listens I need to do.\n",
    "* [ðŸ’™ 2021 by dyl_m](https://www.youtube.com/playlist?list=PLOMUdQFdS-XMaC0KBG2EN8lnwkdShXSk9)\n",
    "* [ðŸ’™ 2022 by dyl_m](https://www.youtube.com/playlist?list=PLOMUdQFdS-XMikLg-T7EuUAnCPdGbTAAJ)\n",
    "* [ðŸ’™ 2023 by dyl_m](https://www.youtube.com/playlist?list=PLOMUdQFdS-XPGM7HHAX9hVb5lVEKSPnK3)\n",
    "* [ðŸ’™ 2024 by dyl_m](https://www.youtube.com/playlist?list=PLOMUdQFdS-XNqUpFzE89aHgwn0wrBidyG)\n",
    "* [ðŸ’™ 2025 by dyl_m](https://www.youtube.com/playlist?list=PLOMUdQFdS-XMfbJk0XdreFpdm2CRCGC-e)"
   ],
   "id": "cc6861abbfc24f74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dependencies",
   "id": "7eaf19a5c12e4c0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.express as px\n",
    "import pyyoutube as pyt\n",
    "import sys\n",
    "\n",
    "import src.youtube as s_yt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "f17b1837f352d4a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "SERVICE = s_yt.create_service_local(log=False)  # Create the YouTube API Client prior any API request",
   "id": "1e0ac28dde72e97e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('../data/pocket_tube.json', 'r', encoding='utf-8') as j_file:\n",
    "    local_db = json.load(j_file)['MUSIQUE']"
   ],
   "id": "39c8f3e6c08e3648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions",
   "id": "99fc276aba2982c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_playlists_content(service: pyt.Client, playlist_id: str) -> list:\n",
    "    \"\"\"Get the videos in a YouTube playlist\n",
    "    :param service: a Python YouTube Client\n",
    "    :param playlist_id: a YouTube playlist ID\n",
    "    :return p_items: playlist items (videos) as a list.\n",
    "    \"\"\"\n",
    "    p_items = []\n",
    "    next_page_token = None\n",
    "    date_format = '%Y-%m-%dT%H:%M:%S%z'\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = service.playlistItems.list(part=['snippet', 'contentDetails'],\n",
    "                                                 playlist_id=playlist_id,\n",
    "                                                 max_results=50,\n",
    "                                                 pageToken=next_page_token)  # Request playlist's items\n",
    "\n",
    "            # Keep necessary data\n",
    "            p_items += [{'video_id': item.contentDetails.videoId,\n",
    "                         'video_title': item.snippet.title,\n",
    "                         'release_date': dt.datetime.strptime(item.contentDetails.videoPublishedAt, date_format) if\n",
    "                         item.contentDetails.videoPublishedAt else None,\n",
    "                         'channel_id': item.snippet.videoOwnerChannelId,\n",
    "                         'channel_name': item.snippet.videoOwnerChannelTitle,\n",
    "                         'playlist_id': playlist_id} for item in request.items]\n",
    "\n",
    "            next_page_token = request.nextPageToken\n",
    "            if next_page_token is None:\n",
    "                break\n",
    "\n",
    "        except pyt.error.PyYouTubeException as error:\n",
    "            print(f'{error.status_code}: {error.message}')\n",
    "            sys.exit()\n",
    "\n",
    "    return p_items"
   ],
   "id": "bbc0485684d8ede3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_channels(service: pyt.Client, channel_list: list) -> list:\n",
    "    \"\"\"Get YouTube channels basic information\n",
    "    :param service: a YouTube service build with 'googleapiclient.discovery'\n",
    "    :param channel_list: list of YouTube channel ID\n",
    "    :return information: a dictionary with channels names, IDs and uploads playlist IDs.\n",
    "    \"\"\"\n",
    "    information = []\n",
    "\n",
    "    # Split task in chunks of size 50 to request on a maximum of 50 channels at each iteration.\n",
    "    channels_chunks = [channel_list[i:i + min(50, len(channel_list))] for i in range(0, len(channel_list), 50)]\n",
    "\n",
    "    for chunk in channels_chunks:\n",
    "        try:\n",
    "            # Request channels\n",
    "            request = service.channels.list(part=['snippet'], channel_id=chunk, max_results=50).items\n",
    "\n",
    "            # Extract upload playlists, channel names and their ID.\n",
    "            information += [{'channel_name': an_item.snippet.title, 'channel_id': an_item.id} for an_item in request]\n",
    "\n",
    "        except pyt.error.PyYouTubeException as error:\n",
    "            print(f'{error.status_code}: {error.message}')\n",
    "            sys.exit()\n",
    "\n",
    "    # Sort by channel name alphabetical order\n",
    "    information = sorted(information, key=lambda dic: dic['channel_name'].lower())\n",
    "\n",
    "    return information"
   ],
   "id": "6faaa6204156112a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reporting\n",
    "\n",
    "### Data collection"
   ],
   "id": "17dc4cdaac2fbac5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Playlist IDs\n",
    "ids = {'music_2021': 'PLOMUdQFdS-XMaC0KBG2EN8lnwkdShXSk9',\n",
    "       'music_2022': 'PLOMUdQFdS-XMikLg-T7EuUAnCPdGbTAAJ',\n",
    "       'music_2023': 'PLOMUdQFdS-XPGM7HHAX9hVb5lVEKSPnK3',\n",
    "       'music_2024': 'PLOMUdQFdS-XNqUpFzE89aHgwn0wrBidyG',\n",
    "       'music_2025': 'PLOMUdQFdS-XMfbJk0XdreFpdm2CRCGC-e',\n",
    "       're_listening': 'PLOMUdQFdS-XP8fi89uBQ5P01DN_9tGJHu'}\n",
    "\n",
    "rev_ids = {value: key for key, value in ids.items()}  # Reversed dict. for labeling"
   ],
   "id": "e1038d2247ed4ae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "music_2021 = pd.DataFrame(get_playlists_content(SERVICE, playlist_id=ids['music_2021']))\n",
    "music_2022 = pd.DataFrame(get_playlists_content(SERVICE, playlist_id=ids['music_2022']))\n",
    "music_2023 = pd.DataFrame(get_playlists_content(SERVICE, playlist_id=ids['music_2023']))\n",
    "music_2024 = pd.DataFrame(get_playlists_content(SERVICE, playlist_id=ids['music_2024']))\n",
    "music_2025 = pd.DataFrame(get_playlists_content(SERVICE, playlist_id=ids['music_2025']))\n",
    "re_listening = pd.DataFrame(get_playlists_content(SERVICE, playlist_id=ids['re_listening']))\n",
    "\n",
    "# All data\n",
    "data = pd.concat([music_2021, music_2022, music_2023, music_2024, music_2025, re_listening]). \\\n",
    "    sort_values(['release_date', 'video_id'], ascending=False, ignore_index=True).dropna()\n",
    "\n",
    "# Without re-listening\n",
    "selection = pd.concat([music_2021, music_2022, music_2023, music_2024, music_2025]). \\\n",
    "    sort_values(['release_date', 'video_id'], ascending=False, ignore_index=True).dropna()\n",
    "\n",
    "data.replace({'playlist_id': rev_ids}, inplace=True)\n",
    "selection.replace({'playlist_id': rev_ids}, inplace=True)"
   ],
   "id": "7e075442b08446e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data",
   "id": "fc6d1027b8fb55e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "selection",
   "id": "2ad7c3ac23d937ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Release Dates Distribution",
   "id": "d00813dfb4610ea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "release_date_his = px.histogram(data, 'release_date', title='Videos Release Dates Distribution', labels={'release_date': 'Release Date'})\n",
    "release_date_his.show()"
   ],
   "id": "1d01f42c81e9ed16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sel_release_date_his = px.histogram(selection, 'release_date', title='Videos Release Dates Distribution',\n",
    "                                    labels={'release_date': 'Release Date'})\n",
    "sel_release_date_his.show()"
   ],
   "id": "7ec2ff50bbf8e0c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Count of videos by channel\n",
    "#### Channel Database"
   ],
   "id": "fc5a0a49f2f00487"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "channel_from_pl = data[['channel_id', 'channel_name']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .sort_values('channel_name', ignore_index=True)\n",
    "\n",
    "channel_from_local = pd.DataFrame(get_channels(SERVICE, local_db))\n",
    "channel_from_local"
   ],
   "id": "cf027bb836a66ae9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Count by videos\n",
    "##### All playlists"
   ],
   "id": "14734761aade002b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chan_count = data.groupby('channel_id')['video_id'].count()\n",
    "\n",
    "chan_names = data[['channel_id', 'channel_name']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .sort_values('channel_name', ignore_index=True)\n",
    "\n",
    "chan_count = pd.DataFrame(chan_count) \\\n",
    "    .merge(chan_names, how='left', on='channel_id')[['channel_id', 'channel_name', 'video_id']] \\\n",
    "    .rename(columns={'video_id': 'n_videos'}) \\\n",
    "    .sort_values('n_videos', ascending=False, ignore_index=True)\n",
    "\n",
    "chan_count"
   ],
   "id": "e486a4a4646969c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Without re-listening",
   "id": "8bd61272f948a2b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sel_chan_count = selection.groupby('channel_id')['video_id'].count()\n",
    "\n",
    "sel_chan_count = pd.DataFrame(sel_chan_count) \\\n",
    "    .merge(chan_names, how='left', on='channel_id')[['channel_id', 'channel_name', 'video_id']] \\\n",
    "    .rename(columns={'video_id': 'n_videos_fil'}) \\\n",
    "    .sort_values('n_videos_fil', ascending=False, ignore_index=True)\n",
    "\n",
    "sel_chan_count"
   ],
   "id": "88ad097ced5f3a34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Weight based on Release Date\n",
    "##### All playlists"
   ],
   "id": "474bbf07bbd6bc7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\n",
    "data['date_weight'] = np.exp(scaler.fit_transform(data.release_date \\\n",
    "                                                  .astype('int') \\\n",
    "                                                  .to_numpy() \\\n",
    "                                                  .reshape(-1, 1)))\n",
    "\n",
    "data_w = data.groupby('channel_id')['date_weight'].sum()\n",
    "\n",
    "data_w = pd.DataFrame(data_w) \\\n",
    "    .merge(chan_names, how='left', on='channel_id')[['channel_id', 'channel_name', 'date_weight']] \\\n",
    "    .sort_values('date_weight', ascending=False, ignore_index=True)\n",
    "\n",
    "data_w"
   ],
   "id": "c02fee6e9282cbc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Without re-listening",
   "id": "8f6a0ae81c76dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selection['date_weight'] = np.exp(scaler.fit_transform(selection.release_date \\\n",
    "                                                       .astype('int') \\\n",
    "                                                       .to_numpy() \\\n",
    "                                                       .reshape(-1, 1)))\n",
    "\n",
    "selection_w = selection.groupby('channel_id')['date_weight'].sum()\n",
    "\n",
    "selection_w = pd.DataFrame(selection_w) \\\n",
    "    .merge(chan_names, how='left', on='channel_id')[['channel_id', 'channel_name', 'date_weight']] \\\n",
    "    .rename(columns={'date_weight': 'date_weight_fil'}) \\\n",
    "    .sort_values('date_weight_fil', ascending=False, ignore_index=True)\n",
    "\n",
    "selection_w"
   ],
   "id": "9e76ed670360200"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### All metrics and classification",
   "id": "73a175f5fe74504"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = chan_count.merge(sel_chan_count, how='left', on=['channel_id', 'channel_name']) \\\n",
    "    .merge(data_w, how='left', on=['channel_id', 'channel_name']) \\\n",
    "    .merge(selection_w, how='left', on=['channel_id', 'channel_name']) \\\n",
    "    .fillna(0) \\\n",
    "    .sort_values(['n_videos', 'n_videos_fil', 'date_weight', 'date_weight_fil'], ascending=False, ignore_index=True)\n",
    "\n",
    "metrics"
   ],
   "id": "c6adf37b6042b2c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### To add to Favorites",
   "id": "796ec49143edd155"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nv_95 = float(metrics.n_videos.quantile(0.95))\n",
    "nvf_95 = float(metrics.n_videos_fil.quantile(0.95))\n",
    "dw_95 = float(metrics.date_weight.quantile(0.95))\n",
    "dwf_95 = float(metrics.date_weight_fil.quantile(0.95))\n",
    "\n",
    "favorites = metrics.loc[(metrics['n_videos'] >= nv_95) &\n",
    "                        (metrics['n_videos_fil'] >= nvf_95) &\n",
    "                        (metrics['date_weight'] >= dw_95) &\n",
    "                        (metrics['date_weight_fil'] >= dwf_95), :]\n",
    "\n",
    "print(favorites[['channel_name', 'channel_id']])"
   ],
   "id": "c3c859f34c560196"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### To add to DB",
   "id": "4f3aa3a332745bbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nv_75 = float(metrics.n_videos.quantile(0.75))\n",
    "nvf_75 = float(metrics.n_videos_fil.quantile(0.75))\n",
    "dw_75 = float(metrics.date_weight.quantile(0.75))\n",
    "dwf_75 = float(metrics.date_weight_fil.quantile(0.75))\n",
    "\n",
    "not_following = metrics.loc[~metrics.channel_id.isin(channel_from_local.channel_id), :]\n",
    "\n",
    "to_follow = not_following.loc[(not_following['n_videos'] > nv_75) &\n",
    "                              (not_following['n_videos_fil'] > nvf_75) &\n",
    "                              (not_following['date_weight'] > dw_75) &\n",
    "                              (not_following['date_weight_fil'] > dwf_75), :]\n",
    "\n",
    "print(to_follow[['channel_name', 'channel_id']])"
   ],
   "id": "758961464cab0835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Uninteresting channels, to delete from DB",
   "id": "607674f503d21ff7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dw_25 = float(metrics.date_weight.quantile(0.25))\n",
    "dwf_25 = float(metrics.date_weight_fil.quantile(0.25))\n",
    "\n",
    "listed = metrics.loc[metrics.channel_id.isin(channel_from_local.channel_id), :]\n",
    "uninteresting = listed.loc[(listed['date_weight'] <= dw_25) &\n",
    "                           (listed['date_weight_fil'] <= dwf_25), :]\n",
    "\n",
    "print(uninteresting[['channel_name', 'channel_id']])"
   ],
   "id": "d454d988bdf84a8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Unlisted in playlists, to delete from DB (?)",
   "id": "ea9b28f86ae9f63e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unlisted = channel_from_local.loc[~channel_from_local.channel_id.isin(metrics.channel_id),:]\n",
    "print(unlisted)"
   ],
   "id": "5875c0d0fd8f413b"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
